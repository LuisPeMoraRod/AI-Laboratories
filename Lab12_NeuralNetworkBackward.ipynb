{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LuisPeMoraRod/AI-Laboratories/blob/ejercicio_3/Lab12_NeuralNetworkBackward.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab - Red Neuronal Backward\n",
        "\n",
        "*   Mauricio Calderón\n",
        "*   Luis Pedro Morales\n"
      ],
      "metadata": {
        "id": "pFZY3P7XzHeK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejercicio 3\n",
        "\n",
        "Programe la red neuronal de la figura del punto 1 sin utilizar métodos ya preexistentes, es decir, programe todo el algoritmo desde cero (sólo el backward). Reutilice el código del forward del laboratorio anterior. Debe usar operaciones con matrices y un mínimo de fors. Prográmelo de forma que sea parametrizable (iteraciones=10000, Alpha=0.5) y pueda hacer el cálculo para cualquier red. Puede modificar la cantidad de iteraciones para determinar si el algoritmo converge.\n",
        "\n",
        "\n",
        "\n",
        "*   Desarrollo del algoritmo según el detalle del punto 2\n",
        "*   Despliegue los valores obtenidos para el primer backward\n",
        "*   Despliegue los pesos óptimos después de modificar la cantidad de iteraciones y el\n",
        "parámetro alpha. Indique en cuál iteración el algoritmo converge.\n",
        "\n"
      ],
      "metadata": {
        "id": "P1YGwgYS2Vcz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from math import exp\n",
        "import numpy as np\n",
        "\n",
        "# Initialize a network using class example as reference\n",
        "def initialize_network():\n",
        "    network = list()\n",
        "    # 2 neurons in hidden layer\n",
        "    bias1 = 0.35\n",
        "    hidden_layer = [{'weights':[0.15, 0.25, bias1]}, {'weights':[0.2, 0.3, bias1]}]\n",
        "    network.append(hidden_layer)\n",
        "\n",
        "    # 2 neurons output layer\n",
        "    bias2 = 0.6\n",
        "    output_layer = [{'weights':[0.4, 0.5, bias2]}, {'weights':[0.45, 0.55, bias2]}]\n",
        "    network.append(output_layer)\n",
        "    return network\n",
        "\n",
        "# Calculate neuron activation for an input\n",
        "def activate(weights, inputs):\n",
        "\tactivation = weights[-1]\n",
        "\tfor i in range(len(weights)-1):\n",
        "\t\tactivation += weights[i] * inputs[i]\n",
        "\treturn activation\n",
        "\n",
        "# Transfer neuron activation\n",
        "def transfer(activation):\n",
        "\treturn 1.0 / (1.0 + exp(-activation))\n",
        "\n",
        "# Forward propagate input to a network output\n",
        "def forward_propagate(network, row):\n",
        "\tinputs = row\n",
        "\tfor layer in network:\n",
        "\t\tnew_inputs = []\n",
        "\t\tfor neuron in layer:\n",
        "\t\t\tactivation = activate(neuron['weights'], inputs)\n",
        "\t\t\tneuron['output'] = transfer(activation)\n",
        "\t\t\tnew_inputs.append(neuron['output'])\n",
        "\t\tinputs = new_inputs\n",
        "\treturn inputs\n",
        "\n",
        "# Calculate the derivative of an neuron output\n",
        "def transfer_derivative(output):\n",
        "\treturn output * (1.0 - output)\n",
        "\n",
        "# Backpropagate error and store in neurons\n",
        "def backward_propagate_error(network, expected):\n",
        "\tfor i in reversed(range(len(network))):\n",
        "\t\tlayer = network[i]\n",
        "\t\terrors = list()\n",
        "\t\tif i != len(network)-1:\n",
        "\t\t\tfor j in range(len(layer)):\n",
        "\t\t\t\terror = 0.0\n",
        "\t\t\t\tfor neuron in network[i + 1]:\n",
        "\t\t\t\t\terror += (neuron['weights'][j] * neuron['delta'])\n",
        "\t\t\t\terrors.append(error)\n",
        "\t\telse:\n",
        "\t\t\tfor j in range(len(layer)):\n",
        "\t\t\t\tneuron = layer[j]\n",
        "\t\t\t\terrors.append(neuron['output'] - expected[j])\n",
        "\t\tfor j in range(len(layer)):\n",
        "\t\t\tneuron = layer[j]\n",
        "\t\t\tneuron['delta'] = errors[j] * transfer_derivative(neuron['output'])\n",
        "\n",
        "# Update network weights with error\n",
        "def update_weights(network, row, l_rate):\n",
        "\tfor i in range(len(network)):\n",
        "\t\tinputs = row[:-1]\n",
        "\t\tif i != 0:\n",
        "\t\t\tinputs = [neuron['output'] for neuron in network[i - 1]]\n",
        "\t\tfor neuron in network[i]:\n",
        "\t\t\tfor j in range(len(inputs)):\n",
        "\t\t\t\tneuron['weights'][j] -= l_rate * neuron['delta'] * inputs[j]\n",
        "\t\t\tneuron['weights'][-1] -= l_rate * neuron['delta']\n",
        "\n",
        "# Train a network for a fixed number of epochs\n",
        "def train_network(network, train, expected, l_rate, n_epoch, tol):\n",
        "    outputs = list()\n",
        "    for _ in range(n_epoch):\n",
        "        sum_error = 0\n",
        "        for row in train:\n",
        "            outputs_i = forward_propagate(network, row)\n",
        "            outputs.append(outputs_i)\n",
        "            sum_error += sum([0.5*(expected[i]-outputs_i[i])**2 for i in range(len(expected))])\n",
        "            backward_propagate_error(network, expected)\n",
        "            update_weights(network, row, l_rate)\n",
        "        if (abs(sum_error) < tol):\n",
        "            break\n",
        "    return outputs\n",
        "\n",
        "# Test training backprop algorithm\n",
        "dataset = [[0.05, 0.1]]\n",
        "expected = [0.01, 0.99]\n",
        "network = initialize_network()\n",
        "\n",
        "alpha = 0.5\n",
        "max_iters = 10000\n",
        "tol = 1e-5\n",
        "outputs = train_network(network, dataset, expected, alpha, max_iters, tol)\n",
        "\n",
        "hidden_layer = network[0]\n",
        "output_layer = network[1]\n",
        "\n",
        "h1 = hidden_layer[0]\n",
        "h2 = hidden_layer[1]\n",
        "o1 = output_layer[0]\n",
        "o2 = output_layer[1]\n",
        "\n",
        "w1 = h1['weights'][0]\n",
        "w2 = h1['weights'][1]\n",
        "w3 = h2['weights'][0]\n",
        "w4 = h2['weights'][1]\n",
        "w5 = o1['weights'][0]\n",
        "w6 = o1['weights'][1]\n",
        "w7 = o2['weights'][0]\n",
        "w8 = o2['weights'][1]\n",
        "\n",
        "first_o1 = outputs[0][0]\n",
        "first_o2 = outputs[0][1]\n",
        "last_o1 = outputs[-1][0]\n",
        "last_o2 = outputs[-1][1]\n",
        "\n",
        "print(f'First iteration: \\n\\t output1 = {first_o1}, output2 = {first_o2}')\n",
        "print(f'Last iteration (n = {len(outputs)-1}): \\n\\t output1 = {last_o1}, output2 = {last_o2}')\n",
        "\n",
        "print(f'\\nHidden layer: \\n\\tw1 = {w1}, w2 = {w2} \\n\\tw3 = {w3}, w4 = {w4}')\n",
        "print(f'Output layer: \\n\\tw5 = {w5}, w6 = {w6} \\n\\tw7 = {w7}, w8 = {w8}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZZ7te2WmKw9",
        "outputId": "f765e881-294c-4b86-cb54-4183712b7337"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First iteration: \n",
            "\t output1 = 0.7569319154399385, output2 = 0.7677178798069613\n",
            "Last iteration (n = 6047): \n",
            "\t output1 = 0.013201450341820496, output2 = 0.9868786195879152\n",
            "\n",
            "Hidden layer: \n",
            "\tw1 = 0.18090462464969917, w2 = 0.25 \n",
            "\tw3 = 0.22936101373817488, w4 = 0.3\n",
            "Output layer: \n",
            "\tw5 = -1.4276542042895422, w6 = -1.318473602965025 \n",
            "\tw7 = 1.4495238306748184, w8 = 1.5431112511817486\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejercicio 4\n",
        "\n",
        "Saque conclusiones de los ejercicios realizados que incluya cambios en la cantidad de\n",
        "iteraciones, cambios en el Alpha, otros que considere importantes"
      ],
      "metadata": {
        "id": "lLrvp6L-zsJt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusiones\n",
        "\n",
        "Para evaluar la influencia del parámetro alpha o tasa de aprendizaje sobre el algoritmo de retropropagación, se ejecutó el método de entrenamiento con diferentes valores de alpha: 0.1 - 0.3 - 0.5 - 0.7 - 0.9. También se incrementó el número de iteraciones máxima a 20000 iteraciones, para ampliar el límite antes de alcanzar la convergencia y favorecer el análisis.\n",
        "\n",
        "A partir de los resultados anteriores se puede demostrar la relación directa de la tasa de aprendizaje  con la velocidad de aprendizaje del algoritmo. En este caso en particular, los menores valores de alpha provocaron que se necesitaran un mayor número de iteraciones para alcanzar la convergencia; al punto tal, que para un alpha de 0.1, ni siquiera se alcanzó la convergencia antes de las 20000 iteraciones. Así, conforme se fue aumentando el alpha fueron dismiuyendo el número de iteraciones para cada convergencia, siendo el alpha de 0.9 la que registró menor cantidad de iteraciones (3360).\n",
        "\n",
        "La tasa de aprendizaje controla la magnitud de las actualizaciones aplicadas a los pesos y sesgos de la red durante el proceso de entrenamiento. Y esto determina qué tan rápido o lento aprende la red a partir de los datos de entrenamiento.\n",
        "\n",
        "Una tasa de aprendizaje más alta puede llevar a una convergencia más rápida inicialmente, ya que los pesos y sesgos se actualizan de manera más significativa en cada iteración. Ahora bien, se debe tener cuidado con no excederse con este valor, ya que si la tasa de aprendizaje es demasiado alta, las actualizaciones pueden volverse demasiado grandes, lo que hace que el algoritmo sobrepase la solución óptima y no logre converger.\n",
        "\n",
        "Otro factor que se puede ver afectado por la tasa de aprendizaje es la estabilidad de la convergencia. Una tasa de aprendizaje más baja puede hacer que la convergencia del algoritmo sea más estable y menos propensa a divergir. Actualizaciones más pequeñas aseguran que los pesos y sesgos se acerquen gradualmente a los valores óptimos, pero esto también puede resultar en una convergencia más lenta."
      ],
      "metadata": {
        "id": "KG8_OUHZz85L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alpha = 0.1\n",
        "max_iters = 20000\n",
        "network = initialize_network()\n",
        "outputs = train_network(network, dataset, expected, alpha, max_iters, tol)\n",
        "last_o1 = outputs[-1][0]\n",
        "last_o2 = outputs[-1][1]\n",
        "print(f'Last iteration (n = {len(outputs)-1}): \\n\\t output1 = {last_o1}, output2 = {last_o2}\\n')\n",
        "\n",
        "alpha = 0.3\n",
        "network = initialize_network()\n",
        "outputs = train_network(network, dataset, expected, alpha, max_iters, tol)\n",
        "last_o1 = outputs[-1][0]\n",
        "last_o2 = outputs[-1][1]\n",
        "print(f'Last iteration (n = {len(outputs)-1}): \\n\\t output1 = {last_o1}, output2 = {last_o2}\\n')\n",
        "\n",
        "alpha = 0.5\n",
        "network = initialize_network()\n",
        "outputs = train_network(network, dataset, expected, alpha, max_iters, tol)\n",
        "last_o1 = outputs[-1][0]\n",
        "last_o2 = outputs[-1][1]\n",
        "print(f'Last iteration (n = {len(outputs)-1}): \\n\\t output1 = {last_o1}, output2 = {last_o2}\\n')\n",
        "\n",
        "alpha = 0.7\n",
        "network = initialize_network()\n",
        "outputs = train_network(network, dataset, expected, alpha, max_iters, tol)\n",
        "last_o1 = outputs[-1][0]\n",
        "last_o2 = outputs[-1][1]\n",
        "print(f'Last iteration (n = {len(outputs)-1}): \\n\\t output1 = {last_o1}, output2 = {last_o2}\\n')\n",
        "\n",
        "alpha = 0.9\n",
        "network = initialize_network()\n",
        "outputs = train_network(network, dataset, expected, alpha, max_iters, tol)\n",
        "last_o1 = outputs[-1][0]\n",
        "last_o2 = outputs[-1][1]\n",
        "print(f'Last iteration (n = {len(outputs)-1}): \\n\\t output1 = {last_o1}, output2 = {last_o2}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Wv_-D_Ezsgz",
        "outputId": "3a40a349-50f2-4bbe-9aa9-f0158f450e5e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last iteration (n = 19999): \n",
            "\t output1 = 0.015079672573649132, output2 = 0.9850426918792227\n",
            "\n",
            "Last iteration (n = 10077): \n",
            "\t output1 = 0.013201689419076547, output2 = 0.9868783549927713\n",
            "\n",
            "Last iteration (n = 6047): \n",
            "\t output1 = 0.013201450341820496, output2 = 0.9868786195879152\n",
            "\n",
            "Last iteration (n = 4319): \n",
            "\t output1 = 0.013201990764072769, output2 = 0.9868781216002792\n",
            "\n",
            "Last iteration (n = 3360): \n",
            "\t output1 = 0.013201381974170708, output2 = 0.9868787487911304\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "jupyter nbconvert --to html /content/Lab12_NeuralNetworkBackward.ipynb"
      ],
      "metadata": {
        "id": "hlAs6ikL0Nej",
        "outputId": "867d116f-b0d3-42bd-c190-4cc4df647b53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NbConvertApp] Converting notebook /content/Lab12_NeuralNetworkBackward.ipynb to html\n",
            "[NbConvertApp] Writing 613071 bytes to /content/Lab12_NeuralNetworkBackward.html\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    }
  ]
}